{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ljYCUgVTOT"
      },
      "source": [
        "**Installing required libraries and keras_ocr**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrj-tZXJt_QE"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall -v \"tensorflow==2.15.1\"\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp4ocwCvJ0FR"
      },
      "outputs": [],
      "source": [
        "!pip install keras-ocr\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeCHCXUfpLPv"
      },
      "outputs": [],
      "source": [
        "import keras_ocr\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZnTKKJTVfjq"
      },
      "source": [
        "Pipeline for OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCxg-QBzstV1",
        "outputId": "c1b306fc-0a86-47ea-9648-83619d0ba935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Downloading /root/.keras-ocr/craft_mlt_25k.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n"
          ]
        }
      ],
      "source": [
        "pipeline = keras_ocr.pipeline.Pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ude6S3BcViH-"
      },
      "source": [
        "Mounting drive for accessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx5JdHGIL02A",
        "outputId": "d3b8ed17-8fdb-4e33-c2c5-404c29e2ac34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import keras_ocr\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hix7c6drVk5r"
      },
      "source": [
        "Dataset collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7kQAi4-N-Ks"
      },
      "outputs": [],
      "source": [
        "german_image = '/content/german_images'\n",
        "english_image = '/content/english_images'\n",
        "\n",
        "\n",
        "german_image_paths = [os.path.join(german_image, f) for f in os.listdir(german_image) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "english_image_paths = [os.path.join(english_image, f) for f in os.listdir(english_image) if f.lower().endswith(('png', 'jpg', 'jpeg'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SshYMfKUZAPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6285824f-0bc4-42e3-e071-38116adce472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/german_images/images (3).jpeg',\n",
              " '/content/german_images/sddefault.jpg',\n",
              " '/content/german_images/18.jpg',\n",
              " '/content/german_images/a6ceca7de0b109cbde662a7aebe1e8ba.jpg',\n",
              " '/content/german_images/images (5).jpeg',\n",
              " '/content/german_images/download (3).png',\n",
              " '/content/german_images/istockphoto-842944206-612x612.jpg',\n",
              " '/content/german_images/1711551786phpLzvlR0.jpeg',\n",
              " '/content/german_images/images (7).jpeg',\n",
              " '/content/german_images/images (11).jpeg',\n",
              " '/content/german_images/images (9).jpeg',\n",
              " '/content/german_images/download (4).png',\n",
              " '/content/german_images/download.png',\n",
              " '/content/german_images/sddefault (1).jpg',\n",
              " '/content/german_images/maxresdefault.jpg',\n",
              " '/content/german_images/6syghe68isg01.jpg',\n",
              " '/content/german_images/images (6).jpeg',\n",
              " '/content/german_images/download (1).png',\n",
              " '/content/german_images/259-large.jpg',\n",
              " '/content/german_images/cbse-class-10-sample-paper-2023-24-german-img.jpg',\n",
              " '/content/german_images/image.png',\n",
              " '/content/german_images/images (10).jpeg',\n",
              " '/content/german_images/images (2).jpeg',\n",
              " '/content/german_images/images (4).jpeg',\n",
              " '/content/german_images/image (1).png',\n",
              " '/content/german_images/images (8).jpeg',\n",
              " '/content/german_images/download.jpeg',\n",
              " '/content/german_images/images.png',\n",
              " '/content/german_images/images (12).jpeg',\n",
              " '/content/german_images/images (1).jpeg',\n",
              " '/content/german_images/download (2).png']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "german_image_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqYkCOwzVp22"
      },
      "source": [
        "If want to access image using URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGVg4sTvBwK"
      },
      "outputs": [],
      "source": [
        "# urls =  [\n",
        "#         'https://fiverr-res.cloudinary.com/images/q_auto,f_auto/gigs/207141041/original/a6894a4cd1db996ad97ba1b8347d986975a12a32/proof-read-your-german-text.png',\n",
        "#         'https://i.pinimg.com/originals/9b/35/ff/9b35ffaeb4de11f88fbf6b2fe860fb12.jpg',\n",
        "#         'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfA3pKDpplSaJwgdx6g35VIj7poJCJgJT_nzQuMo3fSKcKmXeUiH9Ri485szayTjI6c0E&usqp=CAU'\n",
        "#         ]\n",
        "\n",
        "# import requests\n",
        "# indx = 0\n",
        "# for url in urls:\n",
        "#     indx=indx+1\n",
        "#     try:\n",
        "#         response = requests.get(url)\n",
        "#         response.raise_for_status()\n",
        "#         print(f\"URL number : {indx} is accessible.\")\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         print(f\"Error accessing URL number: {indx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SEuYS2Nsjl"
      },
      "outputs": [],
      "source": [
        "prediction_groups = pipeline.recognize(german_image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC92T-qkOAbC"
      },
      "outputs": [],
      "source": [
        "prediction_groups_eng = pipeline.recognize(english_image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBwKoSg8v2SD"
      },
      "outputs": [],
      "source": [
        "# images = [\n",
        "#     keras_ocr.tools.read(url) for url in urls\n",
        "# ]\n",
        "# predicted_groups = pipeline.recognize(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwk2TyKDOvhz"
      },
      "outputs": [],
      "source": [
        "german_image_arrays = []\n",
        "for image_path in german_image_paths:\n",
        "    img = Image.open(image_path)\n",
        "    img_array = np.array(img)\n",
        "    german_image_arrays.append(img_array)\n",
        "english_image_arrays = []\n",
        "for image_path in english_image_paths:\n",
        "    img = Image.open(image_path)\n",
        "    img_array = np.array(img)\n",
        "    english_image_arrays.append(img_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNTDa0qdVtuF"
      },
      "source": [
        "Visualizing german extracted text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Rad8Huw6zL"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=len(german_image_arrays), figsize=(10, 8))\n",
        "for ax, image, predictions in zip(axs, german_image_arrays, prediction_groups):\n",
        "    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iDggYloVxiC"
      },
      "source": [
        "Visualizing english extracted text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1RisDDlPE6G"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=len(english_image_arrays), figsize=(10, 8))\n",
        "for ax, image, predictions in zip(axs, english_image_arrays, prediction_groups_eng):\n",
        "    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-MPVHYV6TR"
      },
      "source": [
        "Sorting via boxes indexes to get the more optimum order of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_3iQuc2xMfN"
      },
      "outputs": [],
      "source": [
        "def extract_paragraphs(predicted_groups):\n",
        "    paragraphs = []\n",
        "    for predicted_image in predicted_groups:\n",
        "        predictions_sorted = sorted(predicted_image, key=lambda x: (min(x[1], key=lambda p: p[1])[1], min(x[1], key=lambda p: p[0])[0]))\n",
        "        paragraph = \"\"\n",
        "        for text, box in predicted_image:\n",
        "            paragraph += text + \" \"\n",
        "\n",
        "        paragraph = paragraph.strip()\n",
        "        paragraphs.append(paragraph)\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "\n",
        "german_paragraphs = extract_paragraphs(prediction_groups)\n",
        "english_paragraphs = extract_paragraphs(prediction_groups_eng)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IQePkEP7fZ"
      },
      "outputs": [],
      "source": [
        "print(len(german_paragraphs))\n",
        "print(len(english_paragraphs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVCL0dfjz4_x"
      },
      "outputs": [],
      "source": [
        "for paragraph in german_paragraphs:\n",
        "  print(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mttG7qp-Qful"
      },
      "outputs": [],
      "source": [
        "for paragraph in english_paragraphs:\n",
        "  print(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2M0daYG4rhi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udps_lemDmdu"
      },
      "outputs": [],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmv6nf9WJ3N"
      },
      "source": [
        "Helsinki-NLP/opus-mt-de-en Model for german to english translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FEaPjb17zMC"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nspg1uqaWR8U"
      },
      "source": [
        "Testing the model with test_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbT1_NpZ9BGt"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"Berlin besitzt neben ausgedehnten Waldgebieten im Westen und Südosten des Stadtgebietes viele große Parkanlagen.\"\n",
        "inputs = tokenizer.encode(test_sentence, return_tensors=\"pt\", truncation=True)\n",
        "outputs = model.generate(inputs)\n",
        "english_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(english_translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm3973XYWVQj"
      },
      "source": [
        "Testing on extracted german text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llM0pEZi9E2S"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(german_paragraphs[0], return_tensors=\"pt\", truncation=True)\n",
        "outputs = model.generate(inputs)\n",
        "english_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(english_translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfHVZ2CZu5p"
      },
      "source": [
        "Corresponding English text extracted using keras_ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWYgcz7KZRNG"
      },
      "outputs": [],
      "source": [
        "print(english_paragraphs[8])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mFL4Ci9KBnah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n"
      ],
      "metadata": {
        "id": "DRPga1zcBnXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"your_dataset.json\")\n",
        "\n",
        "# Splitting dataset into train and validation sets\n",
        "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]\n"
      ],
      "metadata": {
        "id": "xRKzGxO1BnVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Pre-Trained Tokenizer\n",
        "# Using the Helsinki-NLP pre-trained German-English translation model (for example)\n",
        "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the Dataset\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"source\"], text_target=examples[\"target\"], truncation=True)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "RQnkCHkWBnSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load the Pre-Trained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "5N5AO_9efW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "I1kv1ixXfW0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./translation_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500\n",
        ")\n"
      ],
      "metadata": {
        "id": "YCb7QtgufWy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "TvXjEiEMfWxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Train the Model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "njdo4WjmfWvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save the Fine-Tuned Model\n",
        "trainer.save_model(\"./fine_tuned_translation_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_translation_model\")\n"
      ],
      "metadata": {
        "id": "kXaSJ-KjfWtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model fine-tuning complete and saved at './fine_tuned_translation_model'\")"
      ],
      "metadata": {
        "id": "G-Vdq9MAfWrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUa8BzhHfWav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDtoYv-3fWWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LitVHKKPfWUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5-jn2hPfWSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-k0qumJLfWQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "68N2PKbLfWOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}